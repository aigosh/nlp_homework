{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (loss = :mrl, opt = :adagrad, agg = :max, constr = :nneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('news.json', 'r')\n",
    "data = f.readlines()[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [json.loads(i) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [{'title' : i['title'], 'descr' : i['descr']} for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descr</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Следователи Скопинского МСО СУ СК РФ по Рязан...</td>\n",
       "      <td>В рязанской колонии заключенный ударил сотруд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>История Российской Федерации закончится распа...</td>\n",
       "      <td>«Работа такая…» Политолог о заявлениях на Укр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>МОСКВА, 28 фев — РИА Новости. Госкорпорация «...</td>\n",
       "      <td>Роскосмос начал сбор анонимных жалоб на наруш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вашингтон, 28 февраля /Синьхуа/ — Возвращение...</td>\n",
       "      <td>Срочно: Возвращение культурных ценностей в КН...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>МОСКВА, 28 февраля. /ТАСС/. Лекцию о знаменит...</td>\n",
       "      <td>Лекция о кондитерской династии Абрикосовых пр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               descr  \\\n",
       "0   Следователи Скопинского МСО СУ СК РФ по Рязан...   \n",
       "1   История Российской Федерации закончится распа...   \n",
       "2   МОСКВА, 28 фев — РИА Новости. Госкорпорация «...   \n",
       "3   Вашингтон, 28 февраля /Синьхуа/ — Возвращение...   \n",
       "4   МОСКВА, 28 февраля. /ТАСС/. Лекцию о знаменит...   \n",
       "\n",
       "                                               title  \n",
       "0   В рязанской колонии заключенный ударил сотруд...  \n",
       "1   «Работа такая…» Политолог о заявлениях на Укр...  \n",
       "2   Роскосмос начал сбор анонимных жалоб на наруш...  \n",
       "3   Срочно: Возвращение культурных ценностей в КН...  \n",
       "4   Лекция о кондитерской династии Абрикосовых пр...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anastasiapopova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk  \n",
    "nltk.download('stopwords')  \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import PorterStemmer\n",
    "\n",
    "import re \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(document):\n",
    "    document = re.sub(r'\\W', ' ',  document)\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = document.lower()\n",
    "    document = document.split()\n",
    "    document = [lemmatizer.lemmatize(word) for word in document]\n",
    "    return ' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descr</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>следователи скопинского мсо су ск рф по рязанс...</td>\n",
       "      <td>в рязанской колонии заключенный ударил сотрудн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>история российской федерации закончится распад...</td>\n",
       "      <td>работа такая политолог о заявлениях на украине...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>москва 28 фев риа новости госкорпорация роскос...</td>\n",
       "      <td>роскосмос начал сбор анонимных жалоб на наруше...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>вашингтон 28 февраля синьхуа возвращение культ...</td>\n",
       "      <td>срочно возвращение культурных ценностей в кнр ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>москва 28 февраля тасс лекцию о знаменитой мос...</td>\n",
       "      <td>лекция о кондитерской династии абрикосовых про...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               descr  \\\n",
       "0  следователи скопинского мсо су ск рф по рязанс...   \n",
       "1  история российской федерации закончится распад...   \n",
       "2  москва 28 фев риа новости госкорпорация роскос...   \n",
       "3  вашингтон 28 февраля синьхуа возвращение культ...   \n",
       "4  москва 28 февраля тасс лекцию о знаменитой мос...   \n",
       "\n",
       "                                               title  \n",
       "0  в рязанской колонии заключенный ударил сотрудн...  \n",
       "1  работа такая политолог о заявлениях на украине...  \n",
       "2  роскосмос начал сбор анонимных жалоб на наруше...  \n",
       "3  срочно возвращение культурных ценностей в кнр ...  \n",
       "4  лекция о кондитерской династии абрикосовых про...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.title = news.title.apply(lambda x: preprocess_text(x))\n",
    "news.descr = news.descr.apply(lambda x: preprocess_text(x))\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=10)\n",
    "X = vectorizer.fit_transform(news.descr)\n",
    "y = vectorizer.transform(news.title)\n",
    "features = vectorizer.get_feature_names()\n",
    "word2num = {i:indx for indx, i in enumerate(features)} \n",
    "num2word = {indx:i for indx, i in enumerate(features)} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def tokenize_(s):\n",
    "    a = []\n",
    "    for s_ in s.split(' '):\n",
    "        try:\n",
    "            a.append(word2num[s_])\n",
    "        except:\n",
    "            continue\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = news.descr.apply(lambda x: tokenize_(x))\n",
    "#y = news.title.apply(lambda x: tokenize_(x))\n",
    "#X = X.astype(bool).astype(float)\n",
    "#y = y.astype(bool).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 297567), (1000000, 297567))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_hinge(u, v, v_hat, gamma):\n",
    "    loss = gamma - np.dot(u, v) + np.dot(u, v_hat)\n",
    "    if loss > 0:\n",
    "        return (v_hat - v, -u, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://codereview.stackexchange.com/questions/165371/implementing-adagrad-in-python\n",
    "def adagrad_update_numpy(w, h, g_t, mask, lr=0.001, epsilon=1e-7):\n",
    "    h_new = h + g_t * g_t\n",
    "    h_new = np.where(mask, h_new, h)\n",
    "    w_new = w - lr * g_t / (np.sqrt(h_new) + epsilon)\n",
    "    w_new = np.where(mask, w_new, w)\n",
    "    return w_new, h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg(w, x):\n",
    "    EPS = 1e-8\n",
    "    w_filtered_by_words = np.multiply(w, x.todense().transpose()) # тут выбрали веса по тем словам, которые есть\n",
    "    res = w_filtered_by_words.max(axis=0) # посчитали эмбэдинг\n",
    "    # взяли веса, которые равны эмбэдингу и занулили случайные совпадения левых слов\n",
    "    mask = np.multiply((abs(np.vstack([res for _ in range(w.shape[0])]) - w) < EPS), (w_filtered_by_words > 0.))\n",
    "    #print(mask.shape)\n",
    "    return res, mask # mask show values to update\n",
    "\n",
    "def get_element(w, X, i):\n",
    "    return agg(w, X[i]), agg(w, y[i]), agg(w, X[np.random.randint(0, X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_embed = 10\n",
    "weights = np.random.normal(0., 1., size = (X.shape[1], n_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 10), (297567, 10))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg(weights, X[0])[0].shape, agg(weights, X[0])[1].shape # эмбэдинг и маска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "subtracting a sparse matrix from a nonzero scalar is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-5770dd47823d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(x_wrong.shape, mask_wrong.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print('-----------------------')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_hinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_wrong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-87a94883778c>\u001b[0m in \u001b[0;36mbackward_hinge\u001b[0;34m(u, v, v_hat, gamma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackward_hinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             raise NotImplementedError('subtracting a sparse matrix from a '\n\u001b[0m\u001b[1;32m    406\u001b[0m                                       'nonzero scalar is not supported')\n\u001b[1;32m    407\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: subtracting a sparse matrix from a nonzero scalar is not supported"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "gamma = 0.9\n",
    "for epoch in range(n_epoch):\n",
    "    for indx in range(X.shape[0]):\n",
    "        true, y, wrong = get_element(weights, X, indx)\n",
    "        x_true, mask_true = true\n",
    "        y_true, y_mask = y\n",
    "        x_wrong, mask_wrong = wrong\n",
    "        #print(x_true.shape, mask_true.shape)\n",
    "        #print(y_true.shape)\n",
    "        #print(x_wrong.shape, mask_wrong.shape)\n",
    "        #print('-----------------------')\n",
    "        loss = backward_hinge(y_true, x_true, x_wrong, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall_at_k(y_pred, k):\n",
    "    \"\"\"\n",
    "    Calculates recall at k ranking metric.\n",
    "    Args:\n",
    "        y_predicted: Predictions.\n",
    "            Each prediction contains ranking score of all ranking candidates for the particular data sample.\n",
    "            It is supposed that the ranking score for the true candidate goes first in the prediction.\n",
    "    Returns:\n",
    "        Recall at k\n",
    "    \"\"\"\n",
    "    num_examples = float(len(y_pred))\n",
    "    predictions = np.array(y_pred)\n",
    "    predictions = np.flip(np.argsort(predictions, -1), -1)[:, :k]\n",
    "    num_correct = 0\n",
    "    for el in predictions:\n",
    "        if 0 in el:\n",
    "            num_correct += 1\n",
    "    return float(num_correct) / num_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Module\"\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = [input]\n",
    "        for module in self.modules:\n",
    "            self.output.append(module.forward(self.output[-1]))\n",
    "        return self.output[-1]\n",
    "        #return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        self.gradInput =  gradOutput   \n",
    "        for i, module in enumerate(reversed(self.modules)):\n",
    "            self.gradInput = module.backward(self.output[-i-2], self.gradInput)\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()\n",
    "            \n",
    "class Linear(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = #input @ self.W.T + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput @ self.W\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = gradOutput.T @ input\n",
    "        self.gradb = np.sum(gradOutput, axis = 0)\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Criterion\"\n",
    "    \n",
    "# class ClassNLLCriterionUnstable(Criterion):\n",
    "#     EPS = 1e-15\n",
    "#     def __init__(self):\n",
    "#         a = super(ClassNLLCriterionUnstable, self)\n",
    "#         super(ClassNLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "#     def updateOutput(self, input, target):         \n",
    "#         input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "#         self.output = -1/input.shape[0] * np.sum(target * np.log(input_clamp))\n",
    "#         return self.output\n",
    "\n",
    "#     def updateGradInput(self, input, target):\n",
    "#         input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "#         self.gradInput = -target/(input.shape[0] * input_clamp)\n",
    "#         return self.gradInput\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return \"ClassNLLCriterionUnstable\"\n",
    "    \n",
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        self.output = -1/input.shape[0] * np.sum(target * input)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput = -target/input.shape[0]\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
    "- `variables` - list of lists of variables (one list per layer)\n",
    "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
    "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
    "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
    "\n",
    "Formulas for optimizer:\n",
    "\n",
    "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
    "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$ \n",
    "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
    "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam_optimizer(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('m', {})  # first moment vars\n",
    "    state.setdefault('v', {})  # second moment vars\n",
    "    state.setdefault('t', 0)   # timestamp\n",
    "    state['t'] += 1\n",
    "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
    "        assert k in config, config.keys()\n",
    "    \n",
    "    var_index = 0 \n",
    "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "\n",
    "            np.add(var_first_moment * config['beta1'], (1 - config['beta1']) * current_grad, out=var_first_moment)\n",
    "            np.add(var_second_moment * config['beta2'], (1 - config['beta2']) * current_grad * current_grad, out=var_second_moment)\n",
    "            current_var -= lr_t * var_first_moment / (np.sqrt(var_second_moment) + config['epsilon'])\n",
    "                        \n",
    "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
    "            assert var_first_moment is state['m'].get(var_index)\n",
    "            assert var_second_moment is state['v'].get(var_index)\n",
    "            var_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_ = 3 # n layers\n",
    "\n",
    "weights = Sequential()\n",
    "for i in range(len_):\n",
    "    weights.add(Linear(n_in=len(features), n_out=len(features)))\n",
    "#net.add(BatchNormalization(alpha=0.5))\n",
    "#net.add(ChannelwiseScaling(n_out=50))\n",
    "#net.add(ReLU())\n",
    "#net.add(Linear(n_in=50, n_out=10))\n",
    "#net.add(LogSoftMax())\n",
    "\n",
    "#weights = Linear(len(features), len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    weights.zeroGradParameters()\n",
    "    \n",
    "    # те какие-то манипуляции с текстами\n",
    "    indx = np.random.randint(0, X.shape[0])\n",
    "    x_true, y_true, x_wrong = get_element(weights.getParameters()[0], indx):\n",
    "        \n",
    "    #loss = criterion(input1, input2, target)\n",
    "    \n",
    "    #weights.backward(x_batch, criterion.backward(y_pred, y_batch))\n",
    "    \n",
    "    adam_optimizer(weights.getParameters(), \n",
    "                     weights.getGradParameters(), \n",
    "                     config = {'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.9, 'epsilon': 10e-9},\n",
    "                     state = {})   \n",
    "    \n",
    "    # assert контролируем nneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    weights.train()\n",
    "    train_acc_ = 0.\n",
    "    loss_train_ = 0.\n",
    "    if i == 30:\n",
    "        lr /= 10.\n",
    "    for x_batch, y_batch in get_batches((X_train, y_train), batch_size):        \n",
    "        weights.zeroGradParameters()\n",
    "        y_pred = weights.forward(x_batch)\n",
    "        loss = criterion.forward(y_pred, y_batch)\n",
    "        train_acc += accuracy(y_pred, y_batch)\n",
    "        loss_train += loss       \n",
    "        weights.backward(x_batch, criterion.backward(y_pred, y_batch))\n",
    "        adam_optimizer(weights.getParameters(), \n",
    "                     weights.getGradParameters(), \n",
    "                     config = {'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.9, 'epsilon': 10e-9},\n",
    "                     state = {})   \n",
    "    acc_train_ /= len(y_train) / batch_size\n",
    "    return train_acc_, loss_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import MarginRankingLoss\n",
    "from torch.optim import Adagrad\n",
    "\n",
    "weights = [torch.tensor(i).float() for i in weights]\n",
    "sgd_learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "criterization = MarginRankingLoss()\n",
    "optimizer =  Adagrad(weights, sgd_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adagrad_update_numpy(param, accum, g_t, lr=0.001, epsilon=1e-7):\n",
    "      accum_t = accum + g_t * g_t\n",
    "      param_t = param - lr * g_t / (np.sqrt(accum_t) + epsilon)\n",
    "      return param_t, accum_t\n",
    "    \n",
    "def sparse_adagrad_update_numpy(param,\n",
    "                                accum,\n",
    "                                gindexs,\n",
    "                                gvalues,\n",
    "                                lr=0.001,\n",
    "                                epsilon=1e-7):\n",
    "      accum_t = copy.deepcopy(accum)\n",
    "      param_t = copy.deepcopy(param)\n",
    "      # first loop accumulates repeated indices if necessary.\n",
    "      for i in range(len(gindexs)):\n",
    "        gindex = gindexs[i]\n",
    "        gvalue = gvalues[i]\n",
    "        accum_t[gindex] = accum_t[gindex] + gvalue * gvalue\n",
    "      for i in range(len(gindexs)):\n",
    "        gindex = gindexs[i]\n",
    "        gvalue = gvalues[i]\n",
    "        param_t[gindex] = param_t[gindex] - lr * gvalue / (\n",
    "            np.sqrt(accum_t[gindex]) + epsilon)\n",
    "      return param_t, accum_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # те какие-то манипуляции с текстами\n",
    "    \n",
    "    loss = criterization(input1, input2, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # assert l2 == 1 and l2 < числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_reduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-94fc1a5c3b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_reduction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweak_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_reduction'"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "from torch import functional as F\n",
    "from torch import _reduction as _Reduction\n",
    "from torch._jit_internal import weak_module, weak_script_method\n",
    "\n",
    "def margin_ranking_loss(input1, input2, target, margin=0, size_average=None,\n",
    "                        reduce=None, reduction='mean'):\n",
    "    # type: (Tensor, Tensor, Tensor, float, Optional[bool], Optional[bool], str) -> Tensor\n",
    "    r\"\"\"margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
    "    See :class:`~torch.nn.MarginRankingLoss` for details.\n",
    "    \"\"\"  # noqa\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
    "    else:\n",
    "        reduction_enum = _Reduction.get_enum(reduction)\n",
    "    if input1.dim() == 0 or input2.dim() == 0 or target.dim() == 0:\n",
    "        raise RuntimeError((\"margin_ranking_loss does not support scalars, got sizes: \"\n",
    "                            \"input1: {}, input2: {}, target: {} \".format(input1.size(), input2.size(), target.size())))\n",
    "    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n",
    "\n",
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "            \n",
    "class MarginRankingLoss(_Loss):\n",
    "    __constants__ = ['margin', 'reduction']\n",
    "\n",
    "    def __init__(self, margin=0., size_average=None, reduce=None, reduction='mean'):\n",
    "        super(MarginRankingLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.margin = margin\n",
    "\n",
    "    @weak_script_method\n",
    "    def forward(self, input1, input2, target):\n",
    "        return F.margin_ranking_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Iterable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ee201c0267d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"The base class for optimizers.\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mAll\u001b[0m \u001b[0moptimizers\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mimplement\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgeneral\u001b[0m \u001b[0moptimization\u001b[0m \u001b[0mloop\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mlooks\u001b[0m \u001b[0mlike\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-ee201c0267d1>\u001b[0m in \u001b[0;36mOptimizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[1;32m     21\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Iterable' is not defined"
     ]
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"The base class for optimizers.\n",
    "\n",
    "    All optimizers must implement a step() method that updates the parameters.\n",
    "    The general optimization loop then looks like this:\n",
    "\n",
    "    for inputs, targets in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    `zero_grad` initializes the gradients of the parameters to zero. This\n",
    "    allows to accumulate gradients (instead of replacing it) during\n",
    "    backpropagation, which is e.g. useful for skip connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: The parameters to be optimized.\n",
    "        \"\"\"\n",
    "        self._params = params\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update the parameters.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "        for param in self._params:\n",
    "            assert isinstance(param, Parameter)\n",
    "            param.grad = np.zeros_like(param.data)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) optimizer with optional Momentum.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float,\n",
    "                 momentum: Optional[float] = None):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        if momentum:\n",
    "            for param in self._params:\n",
    "                param.state_dict[\"momentum\"] = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        for p in self._params:\n",
    "            if self.momentum:\n",
    "                # update the momentum\n",
    "                p.state_dict[\"momentum\"] *= self.momentum\n",
    "                p.state_dict[\"momentum\"] -= self.lr * p.grad\n",
    "                # update the parameter\n",
    "                p.data += p.state_dict[\"momentum\"]\n",
    "            else:\n",
    "                p.data -= self.lr * p.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
